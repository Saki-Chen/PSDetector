{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from model import SSD300, MultiBoxLoss\n",
    "from datasets import PascalVOCDataset\n",
    "from collections import defaultdict\n",
    "from utils import *\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "\n",
    "# Data parameters\n",
    "data_folder = './'  # folder with data files\n",
    "keep_difficult = True  # use objects considered difficult to detect?\n",
    "# Model parameters\n",
    "# Not too many here since the SSD300 has a very specific structure\n",
    "n_classes = len(label_map)  # number of different types of objects\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Learning parameters\n",
    "checkpoint = None  # path to model checkpoint, None if none\n",
    "batch_size = 16  # batch size\n",
    "start_epoch = 0  # start at this epoch\n",
    "epochs = 200  # number of epochs to run without early-stopping\n",
    "epochs_since_improvement = 0  # number of epochs since there was an improvement in the validation metric\n",
    "best_loss = 100.  # assume a high loss at first\n",
    "workers = 4  # number of workers for loading data in the DataLoader\n",
    "print_freq = 200  # print training or validation status every __ batches\n",
    "lr = 1e-3  # learning rate\n",
    "momentum = 0.9  # momentum\n",
    "weight_decay = 5e-4  # weight decay\n",
    "grad_clip = None  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\n",
    "\n",
    "cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded checkpoint from epoch 84. Best loss so far is 0.259.\n",
      "\n",
      "../TestingPicturesOnModel/IMG_20190220_125237.jpg\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from utils import *\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint = 'BEST_checkpoint_ssd300.pth.tar'\n",
    "checkpoint = torch.load(checkpoint)\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "best_loss = checkpoint['best_loss']\n",
    "print('\\nLoaded checkpoint from epoch %d. Best loss so far is %.3f.\\n' % (start_epoch, best_loss))\n",
    "model = checkpoint['model']\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Transforms\n",
    "resize = transforms.Resize((300, 300))\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "def detect(original_image, min_score, max_overlap, top_k, suppress=None):\n",
    "    \"\"\"\n",
    "    Detect objects in an image with a trained SSD300, and visualize the results.\n",
    "\n",
    "    :param original_image: image, a PIL Image\n",
    "    :param min_score: minimum threshold for a detected box to be considered a match for a certain class\n",
    "    :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via Non-Maximum Suppression (NMS)\n",
    "    :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'\n",
    "    :param suppress: classes that you know for sure cannot be in the image or you do not want in the image, a list\n",
    "    :return: annotated image, a PIL Image\n",
    "    \"\"\"\n",
    "\n",
    "    # Transform\n",
    "    image = normalize(to_tensor(resize(original_image)))\n",
    "\n",
    "    # Move to default device\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Forward prop.\n",
    "    predicted_locs, predicted_scores = model(image.unsqueeze(0))\n",
    "\n",
    "    # Detect objects in SSD output\n",
    "    det_boxes, det_labels, det_scores = model.detect_objects(predicted_locs, predicted_scores, min_score=min_score,\n",
    "                                                             max_overlap=max_overlap, top_k=top_k)\n",
    "\n",
    "    # Move detections to the CPU\n",
    "    det_boxes = det_boxes[0].to('cpu')\n",
    "\n",
    "    # Transform to original image dimensions\n",
    "    original_dims = torch.FloatTensor(\n",
    "        [original_image.width, original_image.height, original_image.width, original_image.height]).unsqueeze(0)\n",
    "    det_boxes = det_boxes * original_dims\n",
    "\n",
    "    # Decode class integer labels\n",
    "    det_labels = [rev_label_map[l] for l in det_labels[0].to('cpu').tolist()]\n",
    "\n",
    "    # If no objects found, the detected labels will be set to ['0.'], i.e. ['background'] in SSD300.detect_objects() in model.py\n",
    "    if det_labels == ['background']:\n",
    "        # Just return original image\n",
    "        return original_image\n",
    "\n",
    "    # Annotate\n",
    "    annotated_image = original_image\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "    font = ImageFont.truetype(\"./calibril.ttf\", 15)\n",
    "\n",
    "    # Suppress specific classes, if needed\n",
    "    for i in range(det_boxes.size(0)):\n",
    "        if suppress is not None:\n",
    "            if det_labels[i] in suppress:\n",
    "                continue\n",
    "\n",
    "        # Boxes\n",
    "        box_location = det_boxes[i].tolist()\n",
    "        draw.rectangle(xy=box_location, outline=label_color_map[det_labels[i]])\n",
    "        draw.rectangle(xy=[l + 1. for l in box_location], outline=label_color_map[\n",
    "            det_labels[i]])  # a second rectangle at an offset of 1 pixel to increase line thickness\n",
    "        # draw.rectangle(xy=[l + 2. for l in box_location], outline=label_color_map[\n",
    "        #     det_labels[i]])  # a third rectangle at an offset of 1 pixel to increase line thickness\n",
    "        # draw.rectangle(xy=[l + 3. for l in box_location], outline=label_color_map[\n",
    "        #     det_labels[i]])  # a fourth rectangle at an offset of 1 pixel to increase line thickness\n",
    "\n",
    "        # Text\n",
    "        text_size = font.getsize(det_labels[i].upper())\n",
    "        text_location = [box_location[0] + 2., box_location[1] - text_size[1]]\n",
    "        textbox_location = [box_location[0], box_location[1] - text_size[1], box_location[0] + text_size[0] + 4.,\n",
    "                            box_location[1]]\n",
    "        draw.rectangle(xy=textbox_location, fill=label_color_map[det_labels[i]])\n",
    "        draw.text(xy=text_location, text=det_labels[i].upper(), fill='white')\n",
    "    del draw\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "from pathlib import Path\n",
    "if __name__ == '__main__':\n",
    "    for img_path in list(Path('../TestingPicturesOnModel').glob('*.jpg')):\n",
    "        print(img_path)\n",
    "        original_image = Image.open(img_path, mode='r')\n",
    "        original_image = original_image.convert('RGB')\n",
    "        detect(original_image, min_score=0.2, max_overlap=0.5, top_k=200).show()\n",
    "        break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PascalVOCDataset(data_folder,\n",
    "                                     split='train',\n",
    "                                     keep_difficult=keep_difficult)\n",
    "val_dataset = PascalVOCDataset(data_folder,\n",
    "                               split='test',\n",
    "                               keep_difficult=keep_difficult)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                           collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
    "                                           pin_memory=True)  # note that we're passing the collate function here\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                         collate_fn=val_dataset.collate_fn, num_workers=workers,\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = iter(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-347b2836e8ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/project/a-PyTorch-Tutorial-to-Object-Detection/datasets.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Apply transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdifficulties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdifficulties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdifficulties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/a-PyTorch-Tutorial-to-Object-Detection/utils.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(image, boxes, labels, difficulties, split)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0mnew_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0;31m# Convert PIL image to Torch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m     \u001b[0mnew_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/a-PyTorch-Tutorial-to-Object-Detection/utils.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, boxes, dims, return_percent_coords)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mnew_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mold_dims\u001b[0m  \u001b[0;31m# percent coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0mnew_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "next(asdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../pklot_non_rotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>occupied</th>\n",
       "      <th>rotated_bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../PKLot/PKLot/UFPR04/Rainy/2013-01-18/2013-01...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[577, 554, 724, 717], [629, 486, 780, 639], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../PKLot/PKLot/UFPR04/Rainy/2013-01-18/2013-01...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[577, 554, 724, 717], [629, 486, 780, 639], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../PKLot/PKLot/UFPR04/Rainy/2013-01-18/2013-01...</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[577, 554, 724, 717], [629, 486, 780, 639], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../PKLot/PKLot/UFPR04/Rainy/2013-01-18/2013-01...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[577, 554, 724, 717], [629, 486, 780, 639], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../PKLot/PKLot/UFPR04/Rainy/2013-01-18/2013-01...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[577, 554, 724, 717], [629, 486, 780, 639], [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  \\\n",
       "0  ../PKLot/PKLot/UFPR04/Rainy/2013-01-18/2013-01...   \n",
       "1  ../PKLot/PKLot/UFPR04/Rainy/2013-01-18/2013-01...   \n",
       "2  ../PKLot/PKLot/UFPR04/Rainy/2013-01-18/2013-01...   \n",
       "3  ../PKLot/PKLot/UFPR04/Rainy/2013-01-18/2013-01...   \n",
       "4  ../PKLot/PKLot/UFPR04/Rainy/2013-01-18/2013-01...   \n",
       "\n",
       "                                            occupied  \\\n",
       "0  [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        rotated_bbox  \n",
       "0  [[577, 554, 724, 717], [629, 486, 780, 639], [...  \n",
       "1  [[577, 554, 724, 717], [629, 486, 780, 639], [...  \n",
       "2  [[577, 554, 724, 717], [629, 486, 780, 639], [...  \n",
       "3  [[577, 554, 724, 717], [629, 486, 780, 639], [...  \n",
       "4  [[577, 554, 724, 717], [629, 486, 780, 639], [...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image_path'] = data['image_path'].map(lambda x: '../' + x)\n",
    "data['rotated_bbox'] = data['rotated_bbox'].map(lambda x: np.array([int(num) for num in x.split()]))\n",
    "data['rotated_bbox'] = data['rotated_bbox'].map(lambda x: np.split(x, len(x) / 4))\n",
    "data['occupied'] = data['occupied'].map(lambda x: [int(num) for num in x.split()])\n",
    "data['image_path'] = data['image_path'].map(lambda x: Path(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pics = Path().glob(\"../PKLot/PKLot/**/*.jpg\")\n",
    "path_to_cnt = defaultdict(int)\n",
    "total = 0\n",
    "for pic in pics:\n",
    "    path_to_cnt[pic.parent] += 1\n",
    "    total +=1\n",
    "    \n",
    "training = []\n",
    "validation = []\n",
    "cnt = 0\n",
    "threshold = .8\n",
    "\n",
    "dates = list(path_to_cnt.keys())\n",
    "shuffle(dates)\n",
    "\n",
    "for pic in dates:\n",
    "    if (cnt < threshold * total):\n",
    "        training.append(pic)\n",
    "        cnt += path_to_cnt[pic]\n",
    "    else:\n",
    "        validation.append(pic)\n",
    "        \n",
    "training = set(training)\n",
    "    \n",
    "def pklot_train_test_split(image_path, train_test_set):\n",
    "    return image_path.parent in train_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data.apply(lambda x: pklot_train_test_split(x['image_path'], training), axis=1)]\n",
    "test_df = data[data.apply(lambda x: pklot_train_test_split(x['image_path'], validation), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
